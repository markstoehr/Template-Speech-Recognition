We begin with stating one of the basic problems encountered in the
pure speech recognition problem.  This back-of-the-envelope comparison
is given in Rabiner.  There are $2^5 = 32$ symbols or so in English
(if you include punctuation) and people tend to speak at a rate below
10 symbols per second, so the information rate is 5 bits per symbol by
10 symbols per second giving 50 bps (bits per second) where we assume
that all bits are independent (this is an upperbound on the
information rate).  If we add in phonemic and prosodic cues (such as
pitch and stress) we can make an argument that the information rate of
speech is upperbounded by 200 bps.

Compare this relatively low information rate to the 64,000 bps that
characterize the infromation rate of the digitized speech waveform.
In order to achieve perceptual fidelity by human listeners (e.g. on a
telephone) the digitized waveform must preserve the frequencies of the
original signal in a bandwidth of 0-4Hz and hence 8,000 samples per
second are required of the analog speech waveform.  The samples can be
quantized on a log scale using 8 bits, thus we have 64,000 bits per
second.  Thus, the task of speech recognition consists in generating a relatively
low information rate signal from a high information rate signal.

* The Source Filter Model

We begin by considering the recognition of vowels. There are a number
of reasons to do this since vowels are generally easier to decode in
noise, additionally, most of the information in vowels is contained in
the 0-1kHz.  If we take the basic problem to be signal processing for analyzing vowels
we can use a basic model of speech called the source filter model.

The motivation is based on the acoustics of vowel production.  The
basic mechanism for the source is the glottal pulse prodced just above
the trachea and before the rest of the vocal tract.  During vowel
sounds the vocal tract is relatively open and can be modeled as a
filter being applied to the source.  This filter has a given set of
resonances which corresponds to the poles of the filter. These
observations form the basis for the LPC signal model.

When the vocal tract is relatively open and when air pressure passing
through is quite large, the pressure waves will resonate at certain
frequencies as a function of the vocal tract shape.  The vocal tract
shape varies as a function of movements from the tongue, lips, and
vellum. By pushing air through the larynx, and rhythmically
contracting and relaxing the glottis the vocal folds vibrate and
produce resonances with parts of the vocal tract. The source, then,
corresponds to the glottal pulse and the filter corresponds to the
vocal tract shape.  The resonant frequencies produced by certain
vocal tract shapes are called formants. This implies that it can be modeled using an
all-pole linear prediction model.

Formants appear to play an import role in the recognition of nearly
all speech sounds. Vowels are often classified in terms of their
formants

* Bank of Filters

Another feature extraction technique is to use a filter bank for
feature processing.  These filters have compact support in time, and
like the LPC coefficients, they are computed over a sliding window
which samples small segments (usually 10-30 ms long) of the speech signal.
The segments are usually sampled every 5ms to 15 ms.

In practice the bank of filters approach is implemented as a
short-time Fourier transform, and we take the magnitude of the
coefficients.  The reason for this relates back to the source-filter
model for vowels: the phase information of the coefficients largely
conveys information about the glottal pulse.  In order to recognize
vowels we are mostly interested in the shape of the vocal tract and at
what frequencies the formants occur at: given that those frequencies
represent the resonances of the vocal tract and hence communicate the
shape of the vocal tract.

Ideally, these filters will give a low-dimensional representation that
just communicates the shape of the vocal tract.  Essentially the same
shape can give rise to some variation of the coefficients so usually
some form of smoothing is applied to the transformed Fourier
coefficients.  Linear smoothing was popular in many old systems, but
it is more common for modern systems to use non-linear
smoothing.

The nonlinear smoothing is usually applied as following: let $X(f)$
be the Although the filter bank might be linear or non-linear
usOften a nonlinear filtering operation is the applied to

We begin by surveying Reddy.  In order to write this review I'm going
to discuss the different features and how they solve different
problems that arise in solving particular problems that arise in
speech recognition.  We begin

-----------------------------------------------------------------------------------------------------------

-----------------------------------------------------------------------------------------------------------
| Task                                    | Mode of Speech   | Vocabulary Size | Take Specific Information | Language                    | Speaker     | Environment |
| Isolated Word Recognition               | Isolated Words   |          10-300 | Limited Use               |                             | Cooperative |             |
| Restricted Connected Speech Recognition | Connected Speech |          30-500 | Limited Use               | Restricted command language | cooperative |             |
|                                         |                  |                 |                           |                             |             |             |



References

* Short Time Fourier Transform

** Two Fundamental Properties L\:{u}tfiye Durak, Orhan Arikan
Basic definition of shift invariance is that if shifts in time and
frequency should should change only the position of time-frequency structure in
a signal and the magnitudes should not be different. Namely if $D_x(t,f)$
is a time frequency representation of $x(t)$ and we have a shifted
version $x_s(t)=x(t-t_s)e^{j2\pi f_s t}$ then we should have
$|D_{x_s}(t,f)| = |D_x(t-t_s,f-f_s)|$ we wish to consider the claim that
magnitude-wise shift-invariance in time requires that $D_x(t,f)$ have
the form
$$ D_x(t,f) = e^{j\hat{\phi}(t,f)} \int \kappa(t-t',f)x(t')dt' $$

The next main idea is that it has rotation invariance, that is that the
fractional fourier transform does not change the time frequency domain
characteristics.  We need to understand what these rotation properties
are and whether these are at all relevant for speech recognition.

*** Proof Of the shift Invariance
This is theorem 1 in Two Fundamental Properties
If a linear system $\mathcal{T}$ satisfies magnitude-wise shift invariance then there exists $h(t)$ and
$\phi(t)$ such that for arbitrary input $x(t)$ we have

$$\mathcal{T}\{x(t)\} = e^{j\hat{phi}(t)}[h(t)*x(t)]$$

We observe that $|\delta(t-t_s)| = |\mathcal{T}\{\delta(t-t_s)\}(t)|$ is fundamentally
what shift invariance means that 

$$\begin{array}{rcl}
 {} |\delta(t-t_s)  | &=&   | \mathcal{T}\{\delta(t-t_s)\}(t) |\\
 {} |\int\delta(t-t_s-t')x(t')  | &=&   | \mathcal{T}\{\delta(t-t_s)\}(t) |
\end{array}$$


Using the Riez representation theorem we know that 
$\mathcal{T}\{x(t)\} = \langle x(t),y(t)\rangle$ for some $y(t)$

** Shift-Variance Analysis of Generalized Sampling Processes

Again we consider the space $L^2$ of square-integrable complex-valued
continuous-time signals.  For a pair of signals $x,\psi\in L^2$ we
can compute an inner product

$$ \langle x,\psi\rangle = \int \psi^*(t)x(t)\;\text{d}t$$

In order to get discrete-time signals from continuous signals
we define a sequence of signals 
$\psi_n(t) = \psi(t-nT)$ were $n\in\mathbb{Z}$, which is essentially
just translates of the base function on an integral lattice.  

Discrete time sampling of a continuous signal may be modeled using this:
namely we can definite a discrete-time signal $y=\{y_n\}_{n\in\mathbb{Z}}$
where $y_n=\langle x,\psi_n\rangle$. In the case where $\psi$ has compact
support this is a windowing operation, in the case where $\psi=\delta$
where $\delta$ is the Dirac $\delta$ this is simply discrete sampling.

We can change the notation for convenience to write $h(t)=\psi^*(-t)$
so $\hat{h}(\omega)=\hat{\psi}^*(\omega)$

* Koloydenko, Amit, Niyogi Features

** Spreading

One line of justification for spreading comes from the neural spike train
literature. In some sense we are mimiccing the response of these 

*** Houghton Spike metric

Neural spike train data comes as arrival times 
$\mathbf{t}=(t_1,t_2,\ldots,t_n)$ and these may be
viewed as a sum of delta spikes
these are then turned into a function
$$\mathcal{F}_h\{\mathbf{t}\}(t) = \sum_{i=0}^n h(t-t_i)$$
where the functions $h$ are kernels.

Spike trains can then be compared using a distance
$$d(\mathbf{t}_1,\mathbf{t}_2;h) = 
    \int_{\mathbb{R}}(\mathcal{F}_h\{\mathbf{t}_1\}(t)
                     -\mathcal{F}_h\{\mathbf{t}_2\}(t))^2\;\text{d}t$$


Another technique is to compute the cost of transforming one
spike train into another.  The transformation may be computed
by moving a spike or by deleting and adding spikes.  In the
Victor-Purpura metric a parameter $q$ is the assigned cost for
moving a spike. So for small $q$ the metric is given by the difference
in the number of spikes (since the cost of moving is small).
When $q$ is large the distance is dominated by how many spikes align
with each other (since you won't move spikes). Setting $q$ trades off
between these two extremes.
* Cepstrum
  The speech signal is often modeled as a convolution between a 
  source in the form of a glottal pulse $e(t)$ and a vocal tract
  shape signal $v(t)$ convolved so that our signal is
  $$ x(t) = e(t)*v(t)$$ which means that in the frequency
  domain we have
  $$ |X(\omega)| = |E(\omega)||V(\omega)|$$
  so in the log domain we have a linear model for the signal
  $$ \log |X(\omega)| = \log |E(\omega)|+ \log|V(\omega)|$$
  and hence linear filtering can recover the parameters more easily.
  
  In the source filter model $V$ varies slowly with $\omega$
  because $v(t)$ has a short impulse response and $E$ varies quickly
  with $\omega$ since $e(t)$ has a long impulse response. This means that, in principle,
  if we take a Fourier Transform
  $$ c(n) = \frac{1}{2\pi}\int_{-\pi}^\pi \log|X(\omega)|e^{j\omega n}\;\operatorname{d}\omega$$
   for smaller values of $n$ the temporal fine structure given by $E(\omega)$ is excluded
   and so only vocal tract information is retained, while if $n$ is large then only
   glottal pulse information is retained and there is little information from the vocal tract shape

* Auditory Filters
  One approach to speech recognition is based filter banks that mimic
  the human auditory system.  These are essentially smoothed estimates
  of the spectrum, however, their motivation relies on psychophysical
  experiments.

** Motivation
   One of the key motivations for viewing the human auditory system as
   a filter comes from masking phenomena.  In one experiment the subject
   is presented with a tone of fixed frequency simultaneously with
   noise in a fixed bandwidth.  The listener 
   experiment proceeds over several trials so that in the first
   trial the tone has to low of an intensity for the subject
   to hear the tone in the noise.  In each subsequent trial
   the tone is presented with greater intensity than in previous trials
   and this intensity is increased until the listener can hear the tone.
   The lowest intensity such that the listener can hear the tone was
   called the threshold intensity.  

   The experiment above was the repeated with multiple different
   bandwidths for the noise.  The experimenters observed that there
   was a critical bandwidth such that the threshold intensity for
   noise was the same if the noise bandwidth was the critical bandwidth
   or greater.  They also observed that if the noise bandwidth was smaller
   than the critical bandwidth the task became easier and the threshold
   intensity dropped.  This critical band is hypothesized to be
   the bandwidth for an auditory filter.
   
   Additionally, it was observed that these implied filters have increasing
   bandwidth.

   The underlying model is that if on has a signal that is a pure tone
   $s(t)$ then the perception of $s(t)$ depends only on the bandpass
   filtered $\mathcal{F}_c[s(t)]$ if the pure tone in $s(t)$ is contained
   within critical band $c$.

   
* Masking Experiments
** Tone Masking
** Forward Spread of Masking
** Upward Spread of Masking  

* Strope 1998
  Spectral cues in the region 400 - 8000 Hz are primarily used for recognition, there is a filter
  bank based on critical band ideas. Additionally logarithmic measures of intensity are used for
  recognition in both Humans and Machines. This reflects work noted in several authors.
  
  There are some areas where humans do something different than
  machines.  The first is that humans allow for separate processing
  across different frequency channels. It is noted in Allen 1994 that
  humans use information from frequency bands that have good signal to
  noise ratios.  ASR systems use time-invariant processing whereas
  humans exhibit context-dependence.  Additionally, ASR systems remove
  pitch information (80-300 Hz) and rely on statistical modeling for
  articulator information(2- 20 Hz)

  The pitch information is contained in the frequency of the glottal
  pulse (the glottus periodically slams shut and opens in the range of
  100-200 Hz).  Harmonics of the fundamental are produced due to the 
  abrupt closure. (?)

  During the production of vowels oen may model the sound as a
  periodic glottal source with the vocal tract forming a filter (thus
  linear prediction is useful).  Other speech sounds such as /s/ are
  formed through turbulence, thus a linear model of speech production
  would include both periodic glottal sources and noise-like sources
  that account for fricatives.

  This work mainly attempts to quantify certain nonlinearities in
  auditory processing and apply it to automatic speech recognition.
  
  There is considerable evidence of dynamic adaptation in the auditory
  system. This is one of the main ideas of this work.  One of the main
  forms of adaption is a decreasing response after the onset
  of a constant stimulus.  Humans show considerable sensitivity to 
  onsets and dynamic spectral cues

** Forward Masking
   Observed in Fletcher, the mechanism appears to be an automatic gain
   control mechanism an exponentially adapting linear offset is added
   to logarithmic energy, This means that if a masking tone is played for
   some period then the threshold for hearing a stimulus tone is higher
   than when there is silence preceding the stimulus.
   
   An experiment was performed to determine the parameters of forward masking
   

** Peak Isolation
   Makes an argument that local spectral peaks are a focus of the auditory
   system particularly during vowels.

* Nelken

  Most strfs show a time and frequency component.
  

* Problem of Speech Recognition


* Picone 1993
  Some discussion is made of sampling and digitizing the speech
  signal.
