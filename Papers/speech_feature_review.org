#+TITLE: Speech Features
#+AUTHOR: Mark Stoehr

We begin with stating one of the basic problems encountered in the
pure speech recognition problem.  This back-of-the-envelope comparison
is given in Rabiner.  There are $2^5 = 32$ symbols or so in English
(if you include punctuation) and people tend to speak at a rate below
10 symbols per second, so the information rate is 5 bits per symbol by
10 symbols per second giving 50 bps (bits per second) where we assume
that all bits are independent (this is an upperbound on the
information rate).  If we add in phonemic and prosodic cues (such as
pitch and stress) we can make an argument that the information rate of
speech is upperbounded by 200 bps.

Compare this relatively low information rate to the 64,000 bps that
characterize the infromation rate of the digitized speech waveform.
In order to achieve perceptual fidelity by human listeners (e.g. on a
telephone) the digitized waveform must preserve the frequencies of the
original signal in a bandwidth of 0-4Hz and hence 8,000 samples per
second are required of the analog speech waveform.  The samples can be
quantized on a log scale using 8 bits, thus we have 64,000 bits per
second.  Thus, the task of speech recognition consists in generating a relatively
low information rate signal from a high information rate signal.

* The Source Filter Model
  
We begin by considering the recognition of vowels. There are a number
of reasons to do this since vowels are generally easier to decode in
noise, additionally, most of the information in vowels is contained in
the 0-1kHz.  If we take the basic problem to be signal processing for analyzing vowels
we can use a basic model of speech called the source filter model.

The motivation is based on the acoustics of vowel production.  The
basic mechanism for the source is the glottal pulse prodced just above
the trachea and before the rest of the vocal tract.  During vowel
sounds the vocal tract is relatively open and can be modeled as a
filter being applied to the source.  This filter has a given set of
resonances which corresponds to the poles of the filter. These
observations form the basis for the LPC signal model.

When the vocal tract is relatively open and when air pressure passing
through is quite large, the pressure waves will resonate at certain
frequencies as a function of the vocal tract shape.  The vocal tract
shape varies as a function of movements from the tongue, lips, and
vellum. By pushing air through the larynx, and rhythmically
contracting and relaxing the glottis the vocal folds vibrate and
produce resonances with parts of the vocal tract. The source, then,
corresponds to the glottal pulse and the filter corresponds to the
vocal tract shape.  The resonant frequencies produced by certain
vocal tract shapes are called formants. This implies that it can be modeled using an
all-pole linear prediction model.

Formants appear to play an import role in the recognition of nearly
all speech sounds. Vowels are often classified in terms of their
formants

* Bank of Filters

Another feature extraction technique is to use a filter bank for
feature processing.  These filters have compact support in time, and
like the LPC coefficients, they are computed over a sliding window
which samples small segments (usually 10-30 ms long) of the speech signal.
The segments are usually sampled every 5ms to 15 ms.

In practice the bank of filters approach is implemented as a
short-time Fourier transform, and we take the magnitude of the
coefficients.  The reason for this relates back to the source-filter
model for vowels: the phase information of the coefficients largely
conveys information about the glottal pulse.  In order to recognize
vowels we are mostly interested in the shape of the vocal tract and at
what frequencies the formants occur at: given that those frequencies
represent the resonances of the vocal tract and hence communicate the
shape of the vocal tract.

Ideally, these filters will give a low-dimensional representation that
just communicates the shape of the vocal tract.  Essentially the same
shape can give rise to some variation of the coefficients so usually
some form of smoothing is applied to the transformed Fourier
coefficients.  Linear smoothing was popular in many old systems, but
it is more common for modern systems to use non-linear
smoothing.

The nonlinear smoothing is usually applied as following: let $X(f)$
be the Although the filter bank might be linear or non-linear
usOften a nonlinear filtering operation is the applied to

We begin by surveying Reddy.  In order to write this review I'm going
to discuss the different features and how they solve different
problems that arise in solving particular problems that arise in
speech recognition.  We begin

-----------------------------------------------------------------------------------------------------------

-----------------------------------------------------------------------------------------------------------
| Task                                    | Mode of Speech   | Vocabulary Size | Take Specific Information | Language                    | Speaker     | Environment |
| Isolated Word Recognition               | Isolated Words   |          10-300 | Limited Use               |                             | Cooperative |             |
| Restricted Connected Speech Recognition | Connected Speech |          30-500 | Limited Use               | Restricted command language | cooperative |             |
|                                         |                  |                 |                           |                             |             |             |



References

* Short Time Fourier Transform

** Two Fundamental Properties L\:{u}tfiye Durak, Orhan Arikan
Basic definition of shift invariance is that if shifts in time and
frequency should should change only the position of time-frequency structure in
a signal and the magnitudes should not be different. Namely if $D_x(t,f)$
is a time frequency representation of $x(t)$ and we have a shifted
version $x_s(t)=x(t-t_s)e^{j2\pi f_s t}$ then we should have
$|D_{x_s}(t,f)| = |D_x(t-t_s,f-f_s)|$ we wish to consider the claim that
magnitude-wise shift-invariance in time requires that $D_x(t,f)$ have
the form
$$ D_x(t,f) = e^{j\hat{\phi}(t,f)} \int \kappa(t-t',f)x(t')dt' $$

The next main idea is that it has rotation invariance, that is that the
fractional fourier transform does not change the time frequency domain
characteristics.  We need to understand what these rotation properties
are and whether these are at all relevant for speech recognition.

*** Proof Of the shift Invariance
This is theorem 1 in Two Fundamental Properties
If a linear system $\mathcal{T}$ satisfies magnitude-wise shift invariance then there exists $h(t)$ and
$\phi(t)$ such that for arbitrary input $x(t)$ we have

$$\mathcal{T}\{x(t)\} = e^{j\hat{phi}(t)}[h(t)*x(t)]$$

We observe that $|\delta(t-t_s)| = |\mathcal{T}\{\delta(t-t_s)\}(t)|$ is fundamentally
what shift invariance means that 

$$\begin{array}{rcl}
 {} |\delta(t-t_s)  | &=&   | \mathcal{T}\{\delta(t-t_s)\}(t) |\\
 {} |\int\delta(t-t_s-t')x(t')  | &=&   | \mathcal{T}\{\delta(t-t_s)\}(t) |
\end{array}$$


Using the Riez representation theorem we know that 
$\mathcal{T}\{x(t)\} = \langle x(t),y(t)\rangle$ for some $y(t)$

** Shift-Variance Analysis of Generalized Sampling Processes

Again we consider the space $L^2$ of square-integrable complex-valued
continuous-time signals.  For a pair of signals $x,\psi\in L^2$ we
can compute an inner product

$$ \langle x,\psi\rangle = \int \psi^*(t)x(t)\;\text{d}t$$

In order to get discrete-time signals from continuous signals
we define a sequence of signals 
$\psi_n(t) = \psi(t-nT)$ were $n\in\mathbb{Z}$, which is essentially
just translates of the base function on an integral lattice.  

Discrete time sampling of a continuous signal may be modeled using this:
namely we can definite a discrete-time signal $y=\{y_n\}_{n\in\mathbb{Z}}$
where $y_n=\langle x,\psi_n\rangle$. In the case where $\psi$ has compact
support this is a windowing operation, in the case where $\psi=\delta$
where $\delta$ is the Dirac $\delta$ this is simply discrete sampling.

We can change the notation for convenience to write $h(t)=\psi^*(-t)$
so $\hat{h}(\omega)=\hat{\psi}^*(\omega)$

* Koloydenko, Amit, Niyogi Features

** Spreading

One line of justification for spreading comes from the neural spike train
literature. In some sense we are mimiccing the response of these 

*** Houghton Spike metric

Neural spike train data comes as arrival times 
$\mathbf{t}=(t_1,t_2,\ldots,t_n)$ and these may be
viewed as a sum of delta spikes
these are then turned into a function
$$\mathcal{F}_h\{\mathbf{t}\}(t) = \sum_{i=0}^n h(t-t_i)$$
where the functions $h$ are kernels.

Spike trains can then be compared using a distance
$$d(\mathbf{t}_1,\mathbf{t}_2;h) = 
    \int_{\mathbb{R}}(\mathcal{F}_h\{\mathbf{t}_1\}(t)
                     -\mathcal{F}_h\{\mathbf{t}_2\}(t))^2\;\text{d}t$$


Another technique is to compute the cost of transforming one
spike train into another.  The transformation may be computed
by moving a spike or by deleting and adding spikes.  In the
Victor-Purpura metric a parameter $q$ is the assigned cost for
moving a spike. So for small $q$ the metric is given by the difference
in the number of spikes (since the cost of moving is small).
When $q$ is large the distance is dominated by how many spikes align
with each other (since you won't move spikes). Setting $q$ trades off
between these two extremes.
* Cepstrum
  The speech signal is often modeled as a convolution between a 
  source in the form of a glottal pulse $e(t)$ and a vocal tract
  shape signal $v(t)$ convolved so that our signal is
  $$ x(t) = e(t)*v(t)$$ which means that in the frequency
  domain we have
  $$ |X(\omega)| = |E(\omega)||V(\omega)|$$
  so in the log domain we have a linear model for the signal
  $$ \log |X(\omega)| = \log |E(\omega)|+ \log|V(\omega)|$$
  and hence linear filtering can recover the parameters more easily.
  
  In the source filter model $V$ varies slowly with $\omega$
  because $v(t)$ has a short impulse response and $E$ varies quickly
  with $\omega$ since $e(t)$ has a long impulse response. This means that, in principle,
  if we take a Fourier Transform
  $$ c(n) = \frac{1}{2\pi}\int_{-\pi}^\pi \log|X(\omega)|e^{j\omega n}\;\operatorname{d}\omega$$
   for smaller values of $n$ the temporal fine structure given by $E(\omega)$ is excluded
   and so only vocal tract information is retained, while if $n$ is large then only
   glottal pulse information is retained and there is little information from the vocal tract shape

* Auditory Filters
  One approach to speech recognition is based filter banks that mimic
  the human auditory system.  These are essentially smoothed estimates
  of the spectrum, however, their motivation relies on psychophysical
  experiments.

** Motivation
   One of the key motivations for viewing the human auditory system as
   a filter comes from masking phenomena.  In one experiment the subject
   is presented with a tone of fixed frequency simultaneously with
   noise in a fixed bandwidth.  The listener 
   experiment proceeds over several trials so that in the first
   trial the tone has to low of an intensity for the subject
   to hear the tone in the noise.  In each subsequent trial
   the tone is presented with greater intensity than in previous trials
   and this intensity is increased until the listener can hear the tone.
   The lowest intensity such that the listener can hear the tone was
   called the threshold intensity.  

   The experiment above was the repeated with multiple different
   bandwidths for the noise.  The experimenters observed that there
   was a critical bandwidth such that the threshold intensity for
   noise was the same if the noise bandwidth was the critical bandwidth
   or greater.  They also observed that if the noise bandwidth was smaller
   than the critical bandwidth the task became easier and the threshold
   intensity dropped.  This critical band is hypothesized to be
   the bandwidth for an auditory filter.
   
   Additionally, it was observed that these implied filters have increasing
   bandwidth.

   The underlying model is that if on has a signal that is a pure tone
   $s(t)$ then the perception of $s(t)$ depends only on the bandpass
   filtered $\mathcal{F}_c[s(t)]$ if the pure tone in $s(t)$ is contained
   within critical band $c$.

   
* Masking Experiments
** Tone Masking
** Forward Spread of Masking
** Upward Spread of Masking  

* Strope 1998
  Spectral cues in the region 400 - 8000 Hz are primarily used for recognition, there is a filter
  bank based on critical band ideas. Additionally logarithmic measures of intensity are used for
  recognition in both Humans and Machines. This reflects work noted in several authors.
  
  There are some areas where humans do something different than
  machines.  The first is that humans allow for separate processing
  across different frequency channels. It is noted in Allen 1994 that
  humans use information from frequency bands that have good signal to
  noise ratios.  ASR systems use time-invariant processing whereas
  humans exhibit context-dependence.  Additionally, ASR systems remove
  pitch information (80-300 Hz) and rely on statistical modeling for
  articulator information(2- 20 Hz)

  The pitch information is contained in the frequency of the glottal
  pulse (the glottus periodically slams shut and opens in the range of
  100-200 Hz).  Harmonics of the fundamental are produced due to the 
  abrupt closure. (?)

  During the production of vowels oen may model the sound as a
  periodic glottal source with the vocal tract forming a filter (thus
  linear prediction is useful).  Other speech sounds such as /s/ are
  formed through turbulence, thus a linear model of speech production
  would include both periodic glottal sources and noise-like sources
  that account for fricatives.

  This work mainly attempts to quantify certain nonlinearities in
  auditory processing and apply it to automatic speech recognition.
  
  There is considerable evidence of dynamic adaptation in the auditory
  system. This is one of the main ideas of this work.  One of the main
  forms of adaption is a decreasing response after the onset
  of a constant stimulus.  Humans show considerable sensitivity to 
  onsets and dynamic spectral cues

** Forward Masking
   Observed in Fletcher, the mechanism appears to be an automatic gain
   control mechanism an exponentially adapting linear offset is added
   to logarithmic energy, This means that if a masking tone is played for
   some period then the threshold for hearing a stimulus tone is higher
   than when there is silence preceding the stimulus.
   
   An experiment was performed to determine the parameters of forward masking
   

** Peak Isolation
   Makes an argument that local spectral peaks are a focus of the auditory
   system particularly during vowels.

* Nelken

  Most strfs show a time and frequency component.
  

* Problem of Speech Recognition


* Picone 1993
  Some discussion is made of sampling and digitizing the speech
  signal.
  
** Introduction
*** Section 1
**** Picone
    Speech recognition is divided into the /signal modeling/ problem and
    the /network searching/ problem.  

    Signal modeling consists in:
    - spectral shaping
    - spectral analysis
    - parametric transformation
    - statistical modeling

    This occupies only 10% of the computation in a modern speech
    recognizer.  Signal models need to be *perceptually meaningful*,
    *invariant*, and reveal *temporal correlations*.

**** Response
    Much of the reason why we only use 10% of the computation for
    signal modeling is because we do such a bad job at it.  There is
    so much to be improved on this front.  Humans need probably a great
    deal less search.  The hope is that by doing a better job at
    signal modeling we will create less of a burden on the recognizers
    to figure out what is going on in a given speech signal.

    Signal modeling that is *perceptually meaningful* is not an
    intrinsic goal by itself, although that would be nice, and it is
    scientifically desirable, this is not what we are aiming for
    generally.  We mosty want good performance, and this means being
    able to be invariant to changes in the signal which do not change
    the linguistic content we care about, while be variant to the the
    content that we do care about.  Conceptually, the invariance and
    temporal correlations criteria give us a way of framing the
    problem. We can create general models for what the invariance
    will mean and what the temporal correlations will mean.

*** Terminology

**** Picone
     Bias in the paper is for speech recognition systems that preserve
     the spectral data, additionally most of the advancement in signal
     models is "empirical optimization", there is very little theory.
     The only way to compare these models is to see their performance
     on different systems.  Most speech recognition systems however
     cannot use multiple different signal models so its very hard to
     do an exhaustive comparison

**** Response
     With Mallat's new work, and some of the discussion about acoustic
     circuits that represent vowels and the like we can actually develop
     invariant theories for the signal models.  Additionally, most of
     these signal models could probably be represented as a deep
     neural network.  Also, the simple binary models that are being used
     count as examples of speech recognition systems that we can use
     potentially to do exhaustive comparisons between different systems.

** Spectral Shaping

*** Picone
    The signal is digitally sampled which introduces 50-60Hz hum and
    attenuates high and low frequencies.  But SNRs around 30 dB are
    possible with modern technology, which is sufficient for good
    recognition.  After the conversion the next main step
    is the application of a preemphasis filter, and its very difficult
    to say exactly why this preemphasis filter approach works.
    The preemphasis filter is generally

    $$ s(t) = s(t) - a_{pre}s(t-1)$$ where $a_{pre}\in [-.1,-.4]$
    usually.  There are a number of ways to motivate this filter.  The
    first of these is due to the physiology of voice production
    (voiced sounds don't get love), the second is due to hearing (the
    preemphasis emphasises perceptually-salient aspects of the signal)
    
    Adaptive preemphasis and more sophisticated algorithms have been
    suggested but these do not get really good performance.

*** Reponse

* Hermansky Spectral Coding
  Bases ideas on something from Dudley that the information content in
  speech is transmitted around 10Hz and it comes from the changes in
  the vocal tract shape.  The idea is to split the signal into several
  spectral energers and have it low-pass filtered at 20Hz, which is
  the basis for the Vocoder

  Speech then was changed during WWII and the post war era
  SpectrographTM technology was employed since one could clearly
  observe vocal tract resonants, and it communicated the shape of the
  vocal tract by where there is resonance. Sonorants could visually
  be well discriminated by the observed sounds.

  During the 1970s the analog SpectrographTM was replaced with digital
  filtering and the fast Fourier transform.  Speech was windowed
  at about 10Hz (reflecting the rate of change of the vocal tract)
  the "sluggish nature" (Dudley) of the muscles that control the
  vocal tract shape.  The resolution of the spectrogram is often
  modified to match human perception (this has recently been shown
  to also accord with the instability of high-frequency estimation).

  Spectrograms have not solved the speech problem. Sonorants are
  affected by coarticulation so that neighboring phones affect the
  spectral shape, different speakers produce phonetically identical
  sounds although the formants may be quite different, a short-time
  spectrum does not explain obstruents whose shape is determined by
  the subsequent sonorant.  Linear filtering can also corrupt the
  observation (its not clear why linear filtering would be common
  enough to make this a serious problem)

  Dynamic features are often added to complement static features.
  The older approaches to speech recognition relied on templates
  which were warped, and these days the dynamical spectral content
  of speech is a nuisance parameter for the model.  Templates
  contain coarticulation data inherently within them, whereas
  HMMs require significant and complicated tweaking
  to manage coarticular, either through complex models or
  by enlargin the state space (thus meaning that more
  parameters need to be stored and estimated).

  The proposal is made to be concerned, instead, with the modulation
  spectrum which was suggest in Houtgast and Steeneken.  The idea here
  is to consider the one dimensional time series $S(\omega_0,t)$ where
  $S(\omega,t)$ is the spectrogram and $\omega_0$ is a fixed
  frequency.  This essentially corresponds to analyzing band-pass
  filtered speech (where the band is centered around $\omega_0$).
  The modulation spectrum at frequency $\omega_0$ is
  $$F(\omega_0,t)= \sum_{\Delta T}( \log S(\omega_0,t) - \frac{1}{T}\sum_{\Delta T}\log S(\omega_0,t))e^{j\omega_0 t}  $$
  
  Experiments by Riezs (1928) suggest that humans are most sensitive to modulations
  in the 2-8Hz range, interestingly most of the energy in the modulation spectrum
  of speech signals is in the 4Hz range (which is the syllabic time scale for speech).

  In experiments by Arai if one projects speech such that the
  modulation spectrum components around 4Hz are zeroed-out then
  intelligibility drops significantly.  Conversely, components < 1Hz
  and > 16Hz have minimal effect on intelligibility. There is also
  biological evidence for band-pass modulation frequency filters.

  An experiment was performed where a vowel was filtered in the
  frequency domain by a filter that had a frequency response the
  inverse of the envelope of the vowels frequency response so
  it became spectrally flat.  Recognition was not impaired in this
  filtered speech, however, the image of the vowel all
  but disappears from the DFT, RASTA PLP however preserves the vowel.

  The longer analysis windows used for these modulation spectrums
  results have to be roughly 250 ms long (to capture 4Hz modulations),
  at the same time the suggestion to use multi-stream ASR
  in the manner suggested by Fletcher was considered at the time.

  One can completely bypass the short time analysis windows all together
  by using FDPLP.
  


